{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ai504_13_gnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamksseo/colab/blob/master/ai504_13_gnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk9dnygJt_nR"
      },
      "source": [
        "# **Week 14: GNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8i13DQduMjT"
      },
      "source": [
        "Today class consists of three things. <br>\n",
        "\n",
        "## **1.  We will Make Graph Convolution Equation.**\n",
        "1-1. We will make graph by using networx libary. <br> \n",
        "1-2. by using Adjacency Matrix, Node index and Node embedding vector from graph, We will follow the aggregation and combination step in Graph Convolution Equation. <br>\n",
        "1-3. Finally We will make GCN layer <br>\n",
        "\n",
        "## **2.  We will make node classification in Cora dataset.**\n",
        "2-1. Cora dataset Information <br>\n",
        "2-2. Implement GCN model with Cora dataset <br>\n",
        "2-3. Visualize node embedding\n",
        "\n",
        "## **3.  (DIY) Run the Graph classification on the Collab Dataset**\n",
        "3-1. I will introduce some brief information about the code and pytorch geometric.  <br><br>\n",
        "\n",
        "If you have any questions, feel free to ask\n",
        "\n",
        "*   E-Mail Address : pacesun@kaist.ac.kr / seongjunyang@kaist.ac.kr\n",
        "* Code made by Seongjun Yang at KAIST GSAI Edlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ52uzalrH8T"
      },
      "source": [
        "## **Prelims** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wG1F_2xo0IP"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ4DTpSuhbxf"
      },
      "source": [
        "!pip install networkx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxC1FIj3JKb"
      },
      "source": [
        "import ipdb\n",
        "import torch\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-awmjVA27QF"
      },
      "source": [
        "### **1. Make Graph Convolution Equation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exwpV6zl2Bjh"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1BqgZ_3xUQ7ScvoPVHUZbx5o4xBGSlLVS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslE1Iny38se"
      },
      "source": [
        "#### **1) Initialize the Graph G**\n",
        "By using networkx library, you can do research in graph or network easily. <br>\n",
        "So, in the Graph Convolution Equation, I'll use networkx libary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7pc-dYFLFxj"
      },
      "source": [
        "#1. Initialize the graph\n",
        "G = nx.Graph(name='G')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhFhYZKrLIZw"
      },
      "source": [
        "#2. Create nodes\n",
        "#In this class, we will make graph that consist of 6 nodes.\n",
        "#Each node is assigned node feature which corresponds to the node name\n",
        "for i in range(1,7):\n",
        "    G.add_node(i, name=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA5OEy53LMB8"
      },
      "source": [
        "#Define the edges and the edges to the graph\n",
        "edges = [(1,2), (1,3), (2,4), (2,5), (3,4), (3,6) ]\n",
        "G.add_edges_from(edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CywDujBwLOGa"
      },
      "source": [
        "#See graph info\n",
        "print('Graph Info:\\n', nx.info(G))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52RnpoFbLWi8"
      },
      "source": [
        "#Inspect the node features\n",
        "print('\\nGraph Nodes: ', G.nodes.data())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dnj1XWUSG4W"
      },
      "source": [
        "#Plot the graph\n",
        "nx.draw(G, with_labels=True, font_weight='bold')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-bCgeY44HRH"
      },
      "source": [
        "#### Inserting Adjacency Matrix to forward pass "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDIQRBxAsDbQ"
      },
      "source": [
        "nx.attr_matrix(G, node_attr='name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpwgjWUj3BKj"
      },
      "source": [
        "#Get the Adjacency Matrix (A) and Node Features Matrix (X) as numpy array\n",
        "A = np.array(nx.attr_matrix(G, node_attr='name')[0]) # Converting for getting numpy Adjacency Matrix (A)\n",
        "X = np.array(nx.attr_matrix(G, node_attr='name')[1]) # Converting for getting numpy Node Features Matrix (X)\n",
        "X = np.expand_dims(X,axis=1) # Make [6, 1] numpy Node Features Matrix (X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPosg1IkLjcz"
      },
      "source": [
        "print('Shape of A: ', A.shape) # [6, 6] matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40OvPaEGLm7v"
      },
      "source": [
        "print('\\nShape of X: ', X.shape) # [6, 1] matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCvbW_emLouR"
      },
      "source": [
        "print('\\nAdjacency Matrix (A):\\n', A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO5FhvnNLqzB"
      },
      "source": [
        "print('\\nNode Features Matrix (X):\\n', X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1Zv86g3BQJ"
      },
      "source": [
        "#Dot product Adjacency Matrix (A) and Node Features (X)\n",
        "AX = np.dot(A,X) # AX\n",
        "print(\"Dot product of A and X (AX):\\n\", AX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txfh9ecN8WN5"
      },
      "source": [
        "##### **Question : Is this the node representations H?**\n",
        "##### No, A*X is just neighbor aggregation.\n",
        "##### **We need the combination step!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0s_dR2n4m0f"
      },
      "source": [
        "#### **Add Self-Loops and Normalize Adjacency Matrix (A)**\n",
        "**A' = A + I**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELrUjRjT3Bei"
      },
      "source": [
        "#Add Self Loops\n",
        "G_self_loops = G.copy() # A'\n",
        "\n",
        "self_loops = []\n",
        "for i in range(1, 1+ G.number_of_nodes()):\n",
        "    self_loops.append((i,i))\n",
        "\n",
        "G_self_loops.add_edges_from(self_loops) # A' = A + I\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxgUsPpEL935"
      },
      "source": [
        "#Check the edges of G_self_loops after adding the self loops\n",
        "print('Edges of G with self-loops:\\n', G_self_loops.edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkxe3s67MANc"
      },
      "source": [
        "#Get the Adjacency Matrix (A) and Node Features Matrix (X) of added self-lopps graph\n",
        "A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0]) # A' numpy Matrix\n",
        "print('Adjacency Matrix of added self-loops G (A_hat):\\n', A_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEnlzJwK9yYX"
      },
      "source": [
        "##### **A' * X Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crADwAUBMCGo"
      },
      "source": [
        "#Calculate the dot product of A_hat and X (AX)\n",
        "A_hatX = np.dot(A_hat, X)\n",
        "print('A_hatX:\\n', A_hatX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYRTZ5m-HE8"
      },
      "source": [
        "##### But, there is another problem.\n",
        "##### Scales of node features differ by the number of neighbors.\n",
        "##### Solution : Normalization by inverse degree matrix.\n",
        "##### **D_inverse * A'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es8HJ5g9J5Gk"
      },
      "source": [
        "#Get the Degree Matrix of the added self-loops graph\n",
        "edge_List = G_self_loops.edges() \n",
        "Deg_Mat = [[i, 0] for i in G_self_loops.nodes()]\n",
        "\n",
        "for element in edge_List:\n",
        "  if element[0] != element[1]:\n",
        "    Deg_Mat[element[0] - 1][1] += 1\n",
        "    Deg_Mat[element[1] - 1][1] += 1\n",
        "  else :\n",
        "    Deg_Mat[element[0] - 1][1] += 1\n",
        "\n",
        "print(Deg_Mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV6cc05jMHx1"
      },
      "source": [
        "#Convert the Degree Matrix to a N x N matrix where N is the number of nodes\n",
        "D = np.diag([deg for [n,deg] in Deg_Mat]) # Get degree matrix\n",
        "\n",
        "print('Degree Matrix of added self-loops G as numpy array (D):\\n', D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eROSLv3__ygH"
      },
      "source": [
        "##### **D_inverse**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg_13neAMJ12"
      },
      "source": [
        "#Get the inverse of Degree Matrix (D)\n",
        "D_inv = np.linalg.inv(D)\n",
        "print('Inverse of D:\\n', D_inv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq-IWob0AQEb"
      },
      "source": [
        "##### **D_inverse*A'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAv47XgDAQnM"
      },
      "source": [
        "D_invA = np.dot(D_inv, A_hat)\n",
        "print(D_invA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu1nCQ3AC82"
      },
      "source": [
        "##### **D_invA'X**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMPl4wN0432i"
      },
      "source": [
        "#Dot product of D and AX for normalization\n",
        "DAX = np.dot(D_invA,X)\n",
        "\n",
        "print('DAXW:\\n', DAX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt2m1K3s5F7L"
      },
      "source": [
        "#### **Add Weights and Activation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qlTPzgG3Bqj"
      },
      "source": [
        "#Initialize the weights\n",
        "np.random.seed(12345)\n",
        "n_h = 4 #number of neurons in the hidden layer\n",
        "n_y = 2 #number of neurons in the output layer\n",
        "W0 = np.random.randn(X.shape[1],n_h) * 0.01\n",
        "W1 = np.random.randn(n_h,n_y) * 0.01\n",
        "\n",
        "print('W0 weight:\\n', W0)\n",
        "print('W1 weight:\\n', W1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMD2Yhali3ZY"
      },
      "source": [
        "#### **GCN Layer**\n",
        "##### **TODO : fill ????? with proper code and run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-db5ci2MsuE"
      },
      "source": [
        "#Implement ReLu as activation function,\n",
        "#Originally, non-linear activation needed, but when I searched some material, relu is used for activate function generally.\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "#Build GCN layer\n",
        "#In this function, we implement numpy to simplify\n",
        "def gcn(A,H,W):\n",
        "    #ipdb.set_trace()\n",
        "    # Make a GCN Layer using the Graph Convolution Equation process so far.\n",
        "    # You can use np.diag, np.sum, np.linalg.inv, np.dot\n",
        "    I =????       # create Identity Matrix of A\n",
        "    A_hat = ???   # add self-loop to A\n",
        "    D = np.diag(np.sum(A_hat, axis=0))  # create Degree Matrix of A\n",
        "    D_inv = np.linalg.inv(D)\n",
        "    D_invA = ???\n",
        "    DAXW = ???\n",
        "    return relu(DAXW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYO2IsTStFXp"
      },
      "source": [
        "#Do forward propagation\n",
        "H1 = gcn(A,X,W0) \n",
        "H2 = gcn(A,H1,W1) \n",
        "print('Node Embedding from GCN output:\\n', H2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb_6f2E25MwO"
      },
      "source": [
        "#### **Plotting Node Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4GiB6AO5R-z"
      },
      "source": [
        "def visualize(h, color):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.xlim([np.min(h[:,0])*0.9, np.max(h[:,0])*1.1])\n",
        "    plt.xlabel('Dimension 0')\n",
        "    plt.ylabel('Dimension 1')\n",
        "\n",
        "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "visualize(H2, color=range(6)) # node3 and node 5 have same embedding, So Two nodes overlap on the screen."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz7DGjMzWIMk"
      },
      "source": [
        "## **2. Node classification on Cora Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NWlhYrOiV7P"
      },
      "source": [
        "### **Prelims**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LipkMKfEWGma"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3uSXXkVmB_5"
      },
      "source": [
        "### **Cora Dataset**\n",
        "Dataset link : https://relational.fit.cvut.cz/dataset/CORA <br>\n",
        "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOaW-DOKkYU6"
      },
      "source": [
        "!wget https://www.dropbox.com/s/fl9mvrio3hah4on/cora.content\n",
        "!wget https://www.dropbox.com/s/l829sldp7xqrt0h/cora.cites"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-IItR-W48eT"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "edgelist = pd.read_csv(os.path.join(\"/content/\", \"cora.cites\"), sep='\\t', header=None, names=[\"target\", \"source\"]) # it has graph\n",
        "edgelist[\"label\"] = \"cites\"\n",
        "edgelist.sample(frac=1).head(5) # <ID of cited paper node> <ID of citing paper node>, by doing this, you can see the edge information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acDPH5FT6Jht",
        "scrolled": true
      },
      "source": [
        "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
        "nx.set_node_attributes(Gnx, \"paper\", \"label\")\n",
        "\n",
        "print(Gnx.nodes) # from edgelist, by using from_pandas_edgelist() function, we can extract node list from edgelist\n",
        "Gnx.nodes[12210] ## by type this, we can see the node feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RrvIYLd8iE6"
      },
      "source": [
        "feature_names = [\"word_{}\".format(ii) for ii in range(1433)]\n",
        "column_names =  feature_names + [\"subject\"]\n",
        "node_data = pd.read_csv(os.path.join(\"/content/\", \"cora.content\"), sep='\\t', header=None, names=column_names)\n",
        "node_data.head(5) # <paper node id> <word_attributes>+ <node label>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzSKBzM8zIO"
      },
      "source": [
        "set(node_data[\"subject\"]) # node class type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiudoKBPBdTM"
      },
      "source": [
        "In the class, we will predict the subject of a paper (node) on the basis of the surrounding node data and the structure of the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6HYsYyKmG5B"
      },
      "source": [
        "### **Hyperparameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dJCt03KbJag"
      },
      "source": [
        "EPOCH = 200\n",
        "SEED = 42\n",
        "NUM_HIDDEN = 16\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3-22nfIjGgU"
      },
      "source": [
        "### **Preprocess and Make Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hy0APqxYSfQ"
      },
      "source": [
        "def encode_onehot(labels): # we will make all class(subject) to one-hot vector for training.\n",
        "    classes = set(labels) # {'Case_Based', 'Genetic_Algorithms', 'Neural_Networks', 'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory'}\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yubfrdUVZu85"
      },
      "source": [
        "def normalize(mx): # This part is similar to the normalization process implemented earlier.\n",
        "    #ipdb.set_trace()\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KbsLOMDZ4HT"
      },
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx): # Convert a scipy sparse matrix to a torch sparse tensor.\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3irNhvUvZGXA"
      },
      "source": [
        "def load_data(path=\"/content/\", dataset=\"cora\"):\n",
        "    # In the function, by using above 3 function, \n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    #ipdb.set_trace()\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str)) # load all tables\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # Compress sparse matrix\n",
        "    labels = encode_onehot(idx_features_labels[:, -1]) # Label onehot encoding\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32) # node list\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    # split all nodes to train/valid/test for node classification\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peM32LKqZGbQ"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDvEaIQHh7Tj"
      },
      "source": [
        "#### **Model Architecture**\n",
        "##### **TODO : Fill ????? with proper code and Run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DkYZGo7W8Ka"
      },
      "source": [
        "class GraphConvolution(Module):\n",
        "    \n",
        "    #Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # initialize weight by using reset_parameters() function\n",
        "        self.in_features = in_features \n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # You can use torch.mm\n",
        "        support = ???? # Make XW  weight = W\n",
        "        output = ???? # Make AXW  adj = A\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCt68pu3XfIT"
      },
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "      # Obtain Node embedding\n",
        "      #ipdb.set_trace()\n",
        "      # Make forward propagation by referencing Section 1 (Graph Convolution Equation's forward propagation).\n",
        "      x = self.gc1(x, adj) # Fisrt GraphConvlution Layer\n",
        "      ???? # relu\n",
        "      ???? # dropout\n",
        "      ???? # Second Graph Convolution Layer\n",
        "      ???? # log(softmax(x))\n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk1563rIhuiJ"
      },
      "source": [
        "#### **Setting for training model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJw6icvtXfhx"
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3grxOsbXfj6"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data() # adj -> adjacency matrix, same ax A,   features -> node feature matrix, same as X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky5HcyaFXfmR"
      },
      "source": [
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1], # [2708, 1433] -> [1433] for matrix multiplication of X and W\n",
        "            nhid=NUM_HIDDEN,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA9MOwNJW8Qq"
      },
      "source": [
        "model.cuda()\n",
        "features = features.cuda()\n",
        "adj = adj.cuda()\n",
        "labels = labels.cuda()\n",
        "idx_train = idx_train.cuda()\n",
        "idx_val = idx_val.cuda()\n",
        "idx_test = idx_test.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJq8sLZpaKlZ"
      },
      "source": [
        "### **Train code**\n",
        "In the train() function, We train GCN by using nll_loss objective function and Adam Optimizer. <br>\n",
        "By using train and validation index, We get output in model result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT9hJiCnalxc"
      },
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPb0nqIWbI7_"
      },
      "source": [
        "### **Test code**\n",
        "In the test() function, we test trained model with node embedding visualization (T-SNE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lSvl3yLaqXb"
      },
      "source": [
        "# Visualize\n",
        "def visualize(h, label, idx):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('Dimension 0')\n",
        "    plt.ylabel('Dimension 1')\n",
        "\n",
        "    h_ = h[idx]\n",
        "    color = [ label[i] for i in idx ]\n",
        "    print(f'Embedding shape: {list(h_.shape)}')\n",
        "    z = TSNE(n_components=2).fit_transform(h_.detach().cpu().numpy())  \n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "def test(): # get loss and accuracy with node embedding visualization\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    visualize(output, labels.detach().cpu(), idx_test)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-PxdS5tbf9q"
      },
      "source": [
        "### **Train**\n",
        "When I measure time for traing, About 1.35 sec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2uHuNHXasOm"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(EPOCH):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTftkRzPbrmG"
      },
      "source": [
        "### **Test**\n",
        "When I measure test time, About 6.79 sec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS2zkuFmal0I"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Testing\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD0fdcPL5XZ1"
      },
      "source": [
        "## **3. (HOMEWORK) Graph Classification on Collab Dataset**\n",
        "\n",
        "**Collab Dataset** <br>\n",
        "it is a large dataset containing many graphs and graph labels. <br>\n",
        "This dataset is mainly used for graph classification.<br> \n",
        "COLLAB is a scientific collaboration dataset. A graph corresponds to a researcher’s ego network,  <br>\n",
        "i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. <br> \n",
        "The code is made on pytorch_geometric library. <br> <br>\n",
        "\n",
        "**Why do I use?** <br>\n",
        "pytorch_geometric is very fast despite working on sparse data. <br>\n",
        "Compared to the Deep GraphLibrary (DGL) 0.1.3, pytorch_geometric trains models up to 15 times faster. <br>\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=13jMho-M4Em8B32HzNWkXpnUcA4QGyXYU)\n",
        "\n",
        "So, I recommend running the code and studying the library. <br><br>\n",
        "\n",
        "Reference : https://medium.com/syncedreview/pytorch-geometric-a-fast-pytorch-library-for-dl-a833dff466e5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV8GudwXVSQn"
      },
      "source": [
        "### **Prem**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKv-FxdnVOdi"
      },
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.9.0+cu111.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my0_ON_jsrkJ"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils import to_networkx\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyWTw6hc-LQ1"
      },
      "source": [
        "%%time\n",
        "\n",
        "def create_one_hot_transform(dataset): # Since the collab dataset does not have a node feature, So I make a node feature using the max_degree value.\n",
        "    max_degree = 0                     # I reference that in https://paperswithcode.com/sota/graph-classification-on-collab.\n",
        "    degs = []\n",
        "    for data in dataset:\n",
        "        degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
        "        max_degree = max(max_degree, degs[-1].max().item())\n",
        "\n",
        "    return T.OneHotDegree(max_degree)\n",
        "\n",
        "def load_dataset():\n",
        "        dataset = TUDataset(root='/tmp/COLLAB', name=\"COLLAB\")\n",
        "        dataset.transform = create_one_hot_transform(dataset)\n",
        "        return dataset\n",
        "\n",
        "dataset = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w50MQPByDH7S"
      },
      "source": [
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "\n",
        "\n",
        "\n",
        "###### One graph #####\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jf3KmCfkgQr"
      },
      "source": [
        "# One graph edges\n",
        "print(data.edge_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgDmiSYohab7"
      },
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "#It shows one graph of Collab dataset.\n",
        "plt.figure(figsize=(10, 10))\n",
        "nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=True, cmap=\"Set2\", width=0.5, node_size=500, node_color='yellow')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDBlvYYTG1Lp"
      },
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle() # Label data are sequentially located. (0, 1, 2)\n",
        "\n",
        "# train / valid\n",
        "train_dataset = dataset[:4000]\n",
        "valid_dataset = dataset[4000:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(valid_dataset)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_OvsocSHUF8"
      },
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Unlike CV and NLP, in graph, DataLoader aggregates node_feature, weight and edge_index from different samples/ graphs into Batches\n",
        "# So The GNN model needs this “batch” information to know which nodes belong to the same graph within a batch to perform computation. \n",
        "# Reference : https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8\n",
        "# Reference : https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UlfB3TOUDDJ"
      },
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels) # When I used one more GCNConv, the performance came out better.\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        #ipdb.set_trace()\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels] , for graph classsification\n",
        "        h = x.clone().detach() # for making graph embedding        \n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x , h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCsUYOZi0Tw8"
      },
      "source": [
        "model = GCN(hidden_channels=64)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch=None):\n",
        "    model.train()\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         data = data.to(device)\n",
        "         out, _ = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "    \n",
        "    print(f'Epoch: {epoch:03d}, Train loss: {loss:.4f}')\n",
        "         \n",
        "\n",
        "def test(loader, visual=False):\n",
        "  model.eval()\n",
        "  \n",
        "  correct = 0\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "    data = data.to(device)\n",
        "    out, h = model(data.x, data.edge_index, data.batch)\n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "    \n",
        "    if visual == True:\n",
        "      colors = ['#3A3120', '#535D8E', '#BD3430']\n",
        "      color = [ colors[i] for i in data.y.detach().cpu()]\n",
        "      z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "      \n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      print(f'Embedding shape: {list(h.shape)}')\n",
        "      plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "      plt.show()\n",
        "\n",
        "  return correct / len(loader.dataset)  # Derive ratio of correct predictions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqvtRCxXFE9U"
      },
      "source": [
        "%%time\n",
        "################################\n",
        "for epoch in range(1, 31):\n",
        "    train(epoch)\n",
        "    test_acc = test(valid_loader)\n",
        "    if epoch % 5 == 0:\n",
        "      print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n68qWvinfz4e"
      },
      "source": [
        "### When you run the code, uncomment below command.\n",
        "#test(valid_loader, visual=True) # t-SNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aMw2S5KdVZ_"
      },
      "source": [
        "## **Reference**\n",
        "Thomas N. Kipf, Max Welling, Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017) <br>\n",
        "http://tkipf.github.io/graph-convolutional-networks/ <br>\n",
        "https://relational.fit.cvut.cz/dataset/CORA <br>\n",
        "https://paperswithcode.com/sota/graph-classification-on-collab <br>\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html <br>\n",
        "https://medium.com/syncedreview/pytorch-geometric-a-fast-pytorch-library-for-dl-a833dff466e5 <br>\n",
        "https://towardsdatascience.com/hands-on-graph-neural-networks-with-pytorch-pytorch-geometric-359487e221a8 <br>\n",
        "https://paperswithcode.com/sota/node-classification-on-cora <br>\n",
        "https://graphsandnetworks.com/the-cora-dataset/ <br>\n",
        "https://github.com/tkipf/pygcn <br>\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/ <br>\n",
        "https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing <br>\n",
        "http://networkrepository.com/COLLAB.php <br>"
      ]
    }
  ]
}