{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_test01.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNW6gNiEcDpTa6wSILCjvpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamksseo/colab/blob/master/pyspark_test01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37mKBVAQOlFM"
      },
      "source": [
        "## Beginner’s Guide To Create PySpark DataFrame\n",
        "ref: https://www.analyticsvidhya.com/blog/2021/09/beginners-guide-to-create-pyspark-dataframe/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNvQXueSOaHn",
        "outputId": "25efd136-fc65-4e3a-cea2-8bf0b2c2f86e"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 65 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=d356b60baa19790721db5a15a418715014288684275a87800a0d3f8cd936fbbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2izHW7QpOvP_"
      },
      "source": [
        "### Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrjb3ZNdOMFO"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3hytK8QO2hB"
      },
      "source": [
        "### Creating a SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pueEUlkeOT-G"
      },
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ2qhV_TO-ph"
      },
      "source": [
        "### Creating SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AbwBeUAOXYf"
      },
      "source": [
        "spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkCtGYqLPG72"
      },
      "source": [
        "### Creating a Resilient Data Structure (RDD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0KntIstPDxS"
      },
      "source": [
        "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JygrLRdEPKnL",
        "outputId": "7d9644ef-2a96-44c0-ca9b-4d8149a9a094"
      },
      "source": [
        "print(type(rdd))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4yjH8F1PdmJ"
      },
      "source": [
        "### Converting the RDD into PySpark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvPLEyvWPOjt"
      },
      "source": [
        "sub = ['Division','English','Mathematics','Physics','Chemistry']\n",
        "marks_df = spark.createDataFrame(rdd, schema=sub)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hlNxvwZPYXF",
        "outputId": "87f63c97-3a90-4816-b4c4-1af8a774cfe2"
      },
      "source": [
        "print(type(marks_df))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJ9HgT3PzXr"
      },
      "source": [
        "### Schema of PySpark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b6j3ktPPj6v",
        "outputId": "27d5febd-7480-4516-843c-7a63140f7ce7"
      },
      "source": [
        "marks_df.printSchema()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Division: string (nullable = true)\n",
            " |-- English: long (nullable = true)\n",
            " |-- Mathematics: long (nullable = true)\n",
            " |-- Physics: long (nullable = true)\n",
            " |-- Chemistry: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPb1RH6ZP7ix"
      },
      "source": [
        "### Contents of PySpark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlpKj5CSP2yw",
        "outputId": "69a7ae50-8b4d-4fe8-cb35-17a07422f060"
      },
      "source": [
        "marks_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+-----------+-------+---------+\n",
            "|Division|English|Mathematics|Physics|Chemistry|\n",
            "+--------+-------+-----------+-------+---------+\n",
            "|       C|     85|         76|     87|       91|\n",
            "|       B|     85|         76|     87|       91|\n",
            "|       A|     85|         78|     96|       92|\n",
            "|       A|     92|         76|     89|       96|\n",
            "+--------+-------+-----------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pML_6B5P-g6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}